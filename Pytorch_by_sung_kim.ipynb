{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w=  0.0\n",
      "\t 1.0 2.0 0.0 4.0\n",
      "\t 2.0 4 0.0 16.0\n",
      "\t 3.0 0 0.0 0.0\n",
      "MSE =  6.666666666666667\n",
      "w=  0.1\n",
      "\t 1.0 2.0 0.1 3.61\n",
      "\t 2.0 4 0.2 14.44\n",
      "\t 3.0 0 0.30000000000000004 0.09000000000000002\n",
      "MSE =  6.046666666666667\n",
      "w=  0.2\n",
      "\t 1.0 2.0 0.2 3.24\n",
      "\t 2.0 4 0.4 12.96\n",
      "\t 3.0 0 0.6000000000000001 0.3600000000000001\n",
      "MSE =  5.5200000000000005\n",
      "w=  0.30000000000000004\n",
      "\t 1.0 2.0 0.30000000000000004 2.8899999999999997\n",
      "\t 2.0 4 0.6000000000000001 11.559999999999999\n",
      "\t 3.0 0 0.9000000000000001 0.8100000000000003\n",
      "MSE =  5.086666666666667\n",
      "w=  0.4\n",
      "\t 1.0 2.0 0.4 2.5600000000000005\n",
      "\t 2.0 4 0.8 10.240000000000002\n",
      "\t 3.0 0 1.2000000000000002 1.4400000000000004\n",
      "MSE =  4.746666666666667\n",
      "w=  0.5\n",
      "\t 1.0 2.0 0.5 2.25\n",
      "\t 2.0 4 1.0 9.0\n",
      "\t 3.0 0 1.5 2.25\n",
      "MSE =  4.5\n",
      "w=  0.6000000000000001\n",
      "\t 1.0 2.0 0.6000000000000001 1.9599999999999997\n",
      "\t 2.0 4 1.2000000000000002 7.839999999999999\n",
      "\t 3.0 0 1.8000000000000003 3.240000000000001\n",
      "MSE =  4.346666666666667\n",
      "w=  0.7000000000000001\n",
      "\t 1.0 2.0 0.7000000000000001 1.6899999999999995\n",
      "\t 2.0 4 1.4000000000000001 6.759999999999998\n",
      "\t 3.0 0 2.1 4.41\n",
      "MSE =  4.286666666666666\n",
      "w=  0.8\n",
      "\t 1.0 2.0 0.8 1.44\n",
      "\t 2.0 4 1.6 5.76\n",
      "\t 3.0 0 2.4000000000000004 5.760000000000002\n",
      "MSE =  4.32\n",
      "w=  0.9\n",
      "\t 1.0 2.0 0.9 1.2100000000000002\n",
      "\t 2.0 4 1.8 4.840000000000001\n",
      "\t 3.0 0 2.7 7.290000000000001\n",
      "MSE =  4.446666666666667\n",
      "w=  1.0\n",
      "\t 1.0 2.0 1.0 1.0\n",
      "\t 2.0 4 2.0 4.0\n",
      "\t 3.0 0 3.0 9.0\n",
      "MSE =  4.666666666666667\n",
      "w=  1.1\n",
      "\t 1.0 2.0 1.1 0.8099999999999998\n",
      "\t 2.0 4 2.2 3.2399999999999993\n",
      "\t 3.0 0 3.3000000000000003 10.890000000000002\n",
      "MSE =  4.98\n",
      "w=  1.2000000000000002\n",
      "\t 1.0 2.0 1.2000000000000002 0.6399999999999997\n",
      "\t 2.0 4 2.4000000000000004 2.5599999999999987\n",
      "\t 3.0 0 3.6000000000000005 12.960000000000004\n",
      "MSE =  5.386666666666668\n",
      "w=  1.3\n",
      "\t 1.0 2.0 1.3 0.48999999999999994\n",
      "\t 2.0 4 2.6 1.9599999999999997\n",
      "\t 3.0 0 3.9000000000000004 15.210000000000003\n",
      "MSE =  5.886666666666668\n",
      "w=  1.4000000000000001\n",
      "\t 1.0 2.0 1.4000000000000001 0.3599999999999998\n",
      "\t 2.0 4 2.8000000000000003 1.4399999999999993\n",
      "\t 3.0 0 4.2 17.64\n",
      "MSE =  6.48\n",
      "w=  1.5\n",
      "\t 1.0 2.0 1.5 0.25\n",
      "\t 2.0 4 3.0 1.0\n",
      "\t 3.0 0 4.5 20.25\n",
      "MSE =  7.166666666666667\n",
      "w=  1.6\n",
      "\t 1.0 2.0 1.6 0.15999999999999992\n",
      "\t 2.0 4 3.2 0.6399999999999997\n",
      "\t 3.0 0 4.800000000000001 23.040000000000006\n",
      "MSE =  7.946666666666669\n",
      "w=  1.7000000000000002\n",
      "\t 1.0 2.0 1.7000000000000002 0.0899999999999999\n",
      "\t 2.0 4 3.4000000000000004 0.3599999999999996\n",
      "\t 3.0 0 5.1000000000000005 26.010000000000005\n",
      "MSE =  8.820000000000002\n",
      "w=  1.8\n",
      "\t 1.0 2.0 1.8 0.03999999999999998\n",
      "\t 2.0 4 3.6 0.15999999999999992\n",
      "\t 3.0 0 5.4 29.160000000000004\n",
      "MSE =  9.786666666666667\n",
      "w=  1.9000000000000001\n",
      "\t 1.0 2.0 1.9000000000000001 0.009999999999999974\n",
      "\t 2.0 4 3.8000000000000003 0.0399999999999999\n",
      "\t 3.0 0 5.7 32.49\n",
      "MSE =  10.846666666666666\n",
      "w=  2.0\n",
      "\t 1.0 2.0 2.0 0.0\n",
      "\t 2.0 4 4.0 0.0\n",
      "\t 3.0 0 6.0 36.0\n",
      "MSE =  12.0\n",
      "w=  2.1\n",
      "\t 1.0 2.0 2.1 0.010000000000000018\n",
      "\t 2.0 4 4.2 0.04000000000000007\n",
      "\t 3.0 0 6.300000000000001 39.69000000000001\n",
      "MSE =  13.24666666666667\n",
      "w=  2.2\n",
      "\t 1.0 2.0 2.2 0.04000000000000007\n",
      "\t 2.0 4 4.4 0.16000000000000028\n",
      "\t 3.0 0 6.6000000000000005 43.56000000000001\n",
      "MSE =  14.586666666666671\n",
      "w=  2.3000000000000003\n",
      "\t 1.0 2.0 2.3000000000000003 0.09000000000000016\n",
      "\t 2.0 4 4.6000000000000005 0.36000000000000065\n",
      "\t 3.0 0 6.9 47.61000000000001\n",
      "MSE =  16.020000000000003\n",
      "w=  2.4000000000000004\n",
      "\t 1.0 2.0 2.4000000000000004 0.16000000000000028\n",
      "\t 2.0 4 4.800000000000001 0.6400000000000011\n",
      "\t 3.0 0 7.200000000000001 51.84000000000002\n",
      "MSE =  17.546666666666674\n",
      "w=  2.5\n",
      "\t 1.0 2.0 2.5 0.25\n",
      "\t 2.0 4 5.0 1.0\n",
      "\t 3.0 0 7.5 56.25\n",
      "MSE =  19.166666666666668\n",
      "w=  2.6\n",
      "\t 1.0 2.0 2.6 0.3600000000000001\n",
      "\t 2.0 4 5.2 1.4400000000000004\n",
      "\t 3.0 0 7.800000000000001 60.84000000000001\n",
      "MSE =  20.880000000000003\n",
      "w=  2.7\n",
      "\t 1.0 2.0 2.7 0.49000000000000027\n",
      "\t 2.0 4 5.4 1.960000000000001\n",
      "\t 3.0 0 8.100000000000001 65.61000000000003\n",
      "MSE =  22.686666666666678\n",
      "w=  2.8000000000000003\n",
      "\t 1.0 2.0 2.8000000000000003 0.6400000000000005\n",
      "\t 2.0 4 5.6000000000000005 2.560000000000002\n",
      "\t 3.0 0 8.4 70.56\n",
      "MSE =  24.58666666666667\n",
      "w=  2.9000000000000004\n",
      "\t 1.0 2.0 2.9000000000000004 0.8100000000000006\n",
      "\t 2.0 4 5.800000000000001 3.2400000000000024\n",
      "\t 3.0 0 8.700000000000001 75.69000000000001\n",
      "MSE =  26.580000000000002\n",
      "w=  3.0\n",
      "\t 1.0 2.0 3.0 1.0\n",
      "\t 2.0 4 6.0 4.0\n",
      "\t 3.0 0 9.0 81.0\n",
      "MSE =  28.666666666666668\n",
      "w=  3.1\n",
      "\t 1.0 2.0 3.1 1.2100000000000002\n",
      "\t 2.0 4 6.2 4.840000000000001\n",
      "\t 3.0 0 9.3 86.49000000000001\n",
      "MSE =  30.846666666666668\n",
      "w=  3.2\n",
      "\t 1.0 2.0 3.2 1.4400000000000004\n",
      "\t 2.0 4 6.4 5.760000000000002\n",
      "\t 3.0 0 9.600000000000001 92.16000000000003\n",
      "MSE =  33.12000000000001\n",
      "w=  3.3000000000000003\n",
      "\t 1.0 2.0 3.3000000000000003 1.6900000000000006\n",
      "\t 2.0 4 6.6000000000000005 6.7600000000000025\n",
      "\t 3.0 0 9.9 98.01\n",
      "MSE =  35.48666666666667\n",
      "w=  3.4000000000000004\n",
      "\t 1.0 2.0 3.4000000000000004 1.960000000000001\n",
      "\t 2.0 4 6.800000000000001 7.840000000000004\n",
      "\t 3.0 0 10.200000000000001 104.04000000000002\n",
      "MSE =  37.94666666666668\n",
      "w=  3.5\n",
      "\t 1.0 2.0 3.5 2.25\n",
      "\t 2.0 4 7.0 9.0\n",
      "\t 3.0 0 10.5 110.25\n",
      "MSE =  40.5\n",
      "w=  3.6\n",
      "\t 1.0 2.0 3.6 2.5600000000000005\n",
      "\t 2.0 4 7.2 10.240000000000002\n",
      "\t 3.0 0 10.8 116.64000000000001\n",
      "MSE =  43.146666666666675\n",
      "w=  3.7\n",
      "\t 1.0 2.0 3.7 2.8900000000000006\n",
      "\t 2.0 4 7.4 11.560000000000002\n",
      "\t 3.0 0 11.100000000000001 123.21000000000004\n",
      "MSE =  45.88666666666668\n",
      "w=  3.8000000000000003\n",
      "\t 1.0 2.0 3.8000000000000003 3.240000000000001\n",
      "\t 2.0 4 7.6000000000000005 12.960000000000004\n",
      "\t 3.0 0 11.4 129.96\n",
      "MSE =  48.720000000000006\n",
      "w=  3.9000000000000004\n",
      "\t 1.0 2.0 3.9000000000000004 3.610000000000001\n",
      "\t 2.0 4 7.800000000000001 14.440000000000005\n",
      "\t 3.0 0 11.700000000000001 136.89000000000001\n",
      "MSE =  51.646666666666675\n",
      "w=  4.0\n",
      "\t 1.0 2.0 4.0 4.0\n",
      "\t 2.0 4 8.0 16.0\n",
      "\t 3.0 0 12.0 144.0\n",
      "MSE =  54.666666666666664\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4,0, 6.0]\n",
    "\n",
    "def forward(x):\n",
    "    return x*w\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)*(y_pred - y)\n",
    "\n",
    "w_list = []\n",
    "mse_list = []\n",
    "\n",
    "for w in np.arange(0.0, 4.1, 0.1):\n",
    "    print(\"w= \", w)\n",
    "    l_sum = 0\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        y_pred_val = forward(x_val)\n",
    "        l = loss(x_val, y_val)\n",
    "        l_sum += l\n",
    "        print(\"\\t\", x_val, y_val, y_pred_val, l)\n",
    "    print(\"MSE = \",l_sum /3)\n",
    "    w_list.append(w)\n",
    "    mse_list.append(l_sum/3)\n",
    "    \n",
    "plt.plot(w_list, mse_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('w')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 4.0\n",
      "\tgrad:  1.0 2.0 -2.0\n",
      "\tgrad:  2.0 4.0 -7.84\n",
      "\tgrad:  3.0 6.0 -16.23\n",
      "progress: 0 w=  1.26 loss=  4.92\n",
      "\tgrad:  1.0 2.0 -1.48\n",
      "\tgrad:  2.0 4.0 -5.8\n",
      "\tgrad:  3.0 6.0 -12.0\n",
      "progress: 1 w=  1.45 loss=  2.69\n",
      "\tgrad:  1.0 2.0 -1.09\n",
      "\tgrad:  2.0 4.0 -4.29\n",
      "\tgrad:  3.0 6.0 -8.87\n",
      "progress: 2 w=  1.6 loss=  1.47\n",
      "\tgrad:  1.0 2.0 -0.81\n",
      "\tgrad:  2.0 4.0 -3.17\n",
      "\tgrad:  3.0 6.0 -6.56\n",
      "progress: 3 w=  1.7 loss=  0.8\n",
      "\tgrad:  1.0 2.0 -0.6\n",
      "\tgrad:  2.0 4.0 -2.34\n",
      "\tgrad:  3.0 6.0 -4.85\n",
      "progress: 4 w=  1.78 loss=  0.44\n",
      "\tgrad:  1.0 2.0 -0.44\n",
      "\tgrad:  2.0 4.0 -1.73\n",
      "\tgrad:  3.0 6.0 -3.58\n",
      "progress: 5 w=  1.84 loss=  0.24\n",
      "\tgrad:  1.0 2.0 -0.33\n",
      "\tgrad:  2.0 4.0 -1.28\n",
      "\tgrad:  3.0 6.0 -2.65\n",
      "progress: 6 w=  1.88 loss=  0.13\n",
      "\tgrad:  1.0 2.0 -0.24\n",
      "\tgrad:  2.0 4.0 -0.95\n",
      "\tgrad:  3.0 6.0 -1.96\n",
      "progress: 7 w=  1.91 loss=  0.07\n",
      "\tgrad:  1.0 2.0 -0.18\n",
      "\tgrad:  2.0 4.0 -0.7\n",
      "\tgrad:  3.0 6.0 -1.45\n",
      "progress: 8 w=  1.93 loss=  0.04\n",
      "\tgrad:  1.0 2.0 -0.13\n",
      "\tgrad:  2.0 4.0 -0.52\n",
      "\tgrad:  3.0 6.0 -1.07\n",
      "progress: 9 w=  1.95 loss=  0.02\n",
      "predict (after training) 4 hours 7.804863933862125\n"
     ]
    }
   ],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = 1.0\n",
    "\n",
    "def forwar(x):\n",
    "    return x*w\n",
    "\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y)*(y_pred -y)\n",
    "\n",
    "def gradient(x,y):\n",
    "    return 2 * x * (x*w-y)\n",
    "\n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        grad = gradient(x_val, y_val)\n",
    "        w = w - 0.01*grad\n",
    "        print(\"\\tgrad: \", x_val, y_val, round(grad,2))\n",
    "        l = loss(x_val, y_val)\n",
    "        \n",
    "    print(\"progress:\", epoch, \"w= \", round(w, 2), \"loss= \", round(l, 2))\n",
    "\n",
    "print(\"predict (after training)\", \"4 hours\", forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor([4.], grad_fn=<MulBackward0>)\n",
      "grad:  1.0 2.0 tensor(-2.)\n",
      "grad:  2.0 4.0 tensor(-7.8400)\n",
      "grad:  3.0 6.0 tensor(-16.2288)\n",
      "progress: 0 tensor(7.3159)\n",
      "grad:  1.0 2.0 tensor(-1.4786)\n",
      "grad:  2.0 4.0 tensor(-5.7962)\n",
      "grad:  3.0 6.0 tensor(-11.9981)\n",
      "progress: 1 tensor(3.9988)\n",
      "grad:  1.0 2.0 tensor(-1.0932)\n",
      "grad:  2.0 4.0 tensor(-4.2852)\n",
      "grad:  3.0 6.0 tensor(-8.8704)\n",
      "progress: 2 tensor(2.1857)\n",
      "grad:  1.0 2.0 tensor(-0.8082)\n",
      "grad:  2.0 4.0 tensor(-3.1681)\n",
      "grad:  3.0 6.0 tensor(-6.5580)\n",
      "progress: 3 tensor(1.1946)\n",
      "grad:  1.0 2.0 tensor(-0.5975)\n",
      "grad:  2.0 4.0 tensor(-2.3422)\n",
      "grad:  3.0 6.0 tensor(-4.8484)\n",
      "progress: 4 tensor(0.6530)\n",
      "grad:  1.0 2.0 tensor(-0.4417)\n",
      "grad:  2.0 4.0 tensor(-1.7316)\n",
      "grad:  3.0 6.0 tensor(-3.5845)\n",
      "progress: 5 tensor(0.3569)\n",
      "grad:  1.0 2.0 tensor(-0.3266)\n",
      "grad:  2.0 4.0 tensor(-1.2802)\n",
      "grad:  3.0 6.0 tensor(-2.6500)\n",
      "progress: 6 tensor(0.1951)\n",
      "grad:  1.0 2.0 tensor(-0.2414)\n",
      "grad:  2.0 4.0 tensor(-0.9465)\n",
      "grad:  3.0 6.0 tensor(-1.9592)\n",
      "progress: 7 tensor(0.1066)\n",
      "grad:  1.0 2.0 tensor(-0.1785)\n",
      "grad:  2.0 4.0 tensor(-0.6997)\n",
      "grad:  3.0 6.0 tensor(-1.4485)\n",
      "progress: 8 tensor(0.0583)\n",
      "grad:  1.0 2.0 tensor(-0.1320)\n",
      "grad:  2.0 4.0 tensor(-0.5173)\n",
      "grad:  3.0 6.0 tensor(-1.0709)\n",
      "progress: 9 tensor(0.0319)\n",
      "predict (after training) 4 tensor([7.8049], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = torch.tensor([1.0],requires_grad = True)\n",
    "\n",
    "def forward(x):\n",
    "    return x*w\n",
    "\n",
    "def loss(x,y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "print(\"predict (before training)\", 4, forward(4))\n",
    "for epoch in range(10):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val)\n",
    "        l.backward()\n",
    "        print(\"grad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data\n",
    "        w.grad.data.zero_()\n",
    "        \n",
    "    print(\"progress:\", epoch, l.data[0])\n",
    "print(\"predict (after training)\", 4, forward(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 38.36743927001953\n",
      "100 0.1311231553554535\n",
      "200 0.030831895768642426\n",
      "300 0.007249763701111078\n",
      "400 0.001704697497189045\n",
      "500 0.0004008332034572959\n",
      "predict (after training) 4 tensor(7.9770)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "x_data = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "y_data = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred\n",
    "    \n",
    "model = MyModel()\n",
    "\n",
    "criterion = torch.nn.MSELoss(size_average = False)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(501):\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    if(epoch%100 == 0):\n",
    "        print(epoch, loss.data.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "hour_var = torch.tensor([[4.0]])\n",
    "print(\"predict (after training)\", 4, model.forward(hour_var).data[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.238405466079712\n",
      "100 0.5854028463363647\n",
      "200 0.5517764687538147\n",
      "300 0.5318406820297241\n",
      "400 0.5135368704795837\n",
      "500 0.4964500367641449\n",
      "600 0.48047712445259094\n",
      "700 0.4655310809612274\n",
      "800 0.45153123140335083\n",
      "900 0.4384031295776367\n",
      "1hour :  tensor(False)\n",
      "7hour :  tensor(True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(1,1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = F.sigmoid(self.linear(x))\n",
    "        return y_pred\n",
    "    \n",
    "x_data = torch.Tensor([[1.0], [2.0], [3.0], [4.0]])\n",
    "y_data = torch.Tensor([[0.],[0.],[1.],[1.]])\n",
    "\n",
    "model = MyModel()\n",
    "criterion = torch.nn.BCELoss(size_average = True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    if(epoch%100 == 0):\n",
    "        print(epoch, loss.data.item())\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "hour_var = torch.Tensor([[1.0]])\n",
    "print(\"1hour : \", model(hour_var).data[0][0]>0.5)\n",
    "hour_var = torch.Tensor([[7.0]])\n",
    "print(\"7hour : \", model(hour_var).data[0][0]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  tensor(0.6488, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6448, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6447, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6446, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6445, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6444, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6442, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6441, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6439, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "loss:  tensor(0.6437, grad_fn=<BinaryCrossEntropyBackward>)\n",
      "496\n",
      "Accuracy :  65.34914361001317\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "xy = np.loadtxt('diabetes.csv.gz', delimiter=',', dtype = np.float32)\n",
    "x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "y_data = torch.from_numpy(xy[:, [-1]])\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8,4)\n",
    "        self.l2 = torch.nn.Linear(4,6)\n",
    "        self.l3 = torch.nn.Linear(6,1)\n",
    "        \n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2))\n",
    "        return y_pred\n",
    "    \n",
    "model = MyModel()\n",
    "\n",
    "cirterion = torch.nn.BCELoss(size_average = True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.1)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    loss = criterion(y_pred, y_data)\n",
    "    \n",
    "    if(epoch%100 == 0):\n",
    "        print(\"loss: \", loss)\n",
    "        \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "cnt = 0\n",
    "for it in range(y_data.size()[0]):\n",
    "    if((y_pred[it][0]>0.5) == y_data[it][0].type(torch.ByteTensor)):\n",
    "        cnt = cnt+1\n",
    "        \n",
    "print(cnt)\n",
    "print(\"Accuracy : \", cnt*100/y_data.size()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.6810482144355774\n",
      "0 1 0.6035415530204773\n",
      "0 2 0.6224713325500488\n",
      "0 3 0.6810896396636963\n",
      "0 4 0.622543454170227\n",
      "0 5 0.6423510909080505\n",
      "0 6 0.6408374905586243\n",
      "0 7 0.6223763227462769\n",
      "0 8 0.6008575558662415\n",
      "0 9 0.6620415449142456\n",
      "0 10 0.7037256360054016\n",
      "0 11 0.6613956093788147\n",
      "0 12 0.5837894082069397\n",
      "0 13 0.6016180515289307\n",
      "0 14 0.7054965496063232\n",
      "0 15 0.7013387680053711\n",
      "0 16 0.5640274882316589\n",
      "0 17 0.6818282008171082\n",
      "0 18 0.7017515301704407\n",
      "0 19 0.6230666637420654\n",
      "0 20 0.5844854712486267\n",
      "0 21 0.7026364803314209\n",
      "0 22 0.6406668424606323\n",
      "0 23 0.6166762709617615\n",
      "1 0 0.5220386981964111\n",
      "1 1 0.6839430332183838\n",
      "1 2 0.6208837628364563\n",
      "1 3 0.5981848835945129\n",
      "1 4 0.555823028087616\n",
      "1 5 0.5975558757781982\n",
      "1 6 0.527087926864624\n",
      "1 7 0.5955474376678467\n",
      "1 8 0.7174712419509888\n",
      "1 9 0.7362598180770874\n",
      "1 10 0.75254225730896\n",
      "1 11 0.6208395957946777\n",
      "1 12 0.7253329157829285\n",
      "1 13 0.5813343524932861\n",
      "1 14 0.6819541454315186\n",
      "1 15 0.6216100454330444\n",
      "1 16 0.6224380135536194\n",
      "1 17 0.6420544385910034\n",
      "1 18 0.6208305954933167\n",
      "1 19 0.7257076501846313\n",
      "1 20 0.7011458873748779\n",
      "1 21 0.586338460445404\n",
      "1 22 0.7214681506156921\n",
      "1 23 0.7223977446556091\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiabetesDataset(Dataset):\n",
    "    \n",
    "    def __init__(self):\n",
    "        xy = np.loadtxt('diabetes.csv.gz', delimiter = ',', dtype = np.float32)\n",
    "        \n",
    "        self.len = xy.shape[0]\n",
    "        self.x_data = torch.from_numpy(xy[:, 0:-1])\n",
    "        self.y_data = torch.from_numpy(xy[:, [-1]])\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "dataset = DiabetesDataset()\n",
    "train_loader = DataLoader(dataset = dataset,\n",
    "                         batch_size = 32,\n",
    "                         shuffle = True,\n",
    "                         num_workers = 0)\n",
    "\n",
    "for epoch in range(2):\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        y_pred = model(inputs)\n",
    "        \n",
    "        loss = criterion(y_pred, labels)\n",
    "        print(epoch, i, loss.data.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.1518) tensor(0.4812)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "y = torch.LongTensor([2, 0, 1])\n",
    "y.requires_grad = False\n",
    "\n",
    "y_pred1 = torch.Tensor([[2.0, 1.0, 0.1],\n",
    "                      [1.0, 1.4, 0.1],\n",
    "                      [1.1, 0.2, 3.1]])\n",
    "y_pred2 = torch.Tensor([[0.5, 1.0, 2.5],\n",
    "                      [2.2, 1.1, 0.3],\n",
    "                      [1.5, 2.0, 1.3]])\n",
    "\n",
    "l1 = loss(y_pred1, y)\n",
    "l2 = loss(y_pred2, y)\n",
    "\n",
    "print(l1, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303826\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.298490\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.299068\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.301366\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 2.289932\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.292257\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 2.285275\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 2.287439\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 2.285402\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 2.281440\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.277319\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 2.265400\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 2.241256\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 2.231885\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 2.201541\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.143687\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.059861\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 1.874338\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 1.700342\n",
      "\n",
      "Test set: Average loss: 0.0224, Accuracy: 5770/10000 (58%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.509591\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 1.228554\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.951873\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.906848\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.565563\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.689121\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.679542\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.576087\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.747830\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.636735\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.549399\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.457499\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.339929\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.578934\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.721857\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.514472\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.385051\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.580875\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.548447\n",
      "\n",
      "Test set: Average loss: 0.0067, Accuracy: 8759/10000 (88%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.415178\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.340437\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.540481\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.368462\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.433086\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.385462\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.358236\n",
      "Train Epoch: 3 [22400/60000 (37%)]\tLoss: 0.339839\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.737044\n",
      "Train Epoch: 3 [28800/60000 (48%)]\tLoss: 0.228074\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.442813\n",
      "Train Epoch: 3 [35200/60000 (59%)]\tLoss: 0.394340\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.342850\n",
      "Train Epoch: 3 [41600/60000 (69%)]\tLoss: 0.255536\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.368585\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.365352\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.416474\n",
      "Train Epoch: 3 [54400/60000 (91%)]\tLoss: 0.236671\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.408206\n",
      "\n",
      "Test set: Average loss: 0.0050, Accuracy: 9074/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.243142\n",
      "Train Epoch: 4 [3200/60000 (5%)]\tLoss: 0.314664\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.235711\n",
      "Train Epoch: 4 [9600/60000 (16%)]\tLoss: 0.195243\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.231220\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.247863\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.327135\n",
      "Train Epoch: 4 [22400/60000 (37%)]\tLoss: 0.294793\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.163990\n",
      "Train Epoch: 4 [28800/60000 (48%)]\tLoss: 0.129816\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.198196\n",
      "Train Epoch: 4 [35200/60000 (59%)]\tLoss: 0.236400\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.245274\n",
      "Train Epoch: 4 [41600/60000 (69%)]\tLoss: 0.203912\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.335551\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.184187\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.191748\n",
      "Train Epoch: 4 [54400/60000 (91%)]\tLoss: 0.238380\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.296331\n",
      "\n",
      "Test set: Average loss: 0.0036, Accuracy: 9374/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.241948\n",
      "Train Epoch: 5 [3200/60000 (5%)]\tLoss: 0.311165\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.307942\n",
      "Train Epoch: 5 [9600/60000 (16%)]\tLoss: 0.087005\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.246139\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.243027\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.194152\n",
      "Train Epoch: 5 [22400/60000 (37%)]\tLoss: 0.134895\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.150085\n",
      "Train Epoch: 5 [28800/60000 (48%)]\tLoss: 0.217521\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.226496\n",
      "Train Epoch: 5 [35200/60000 (59%)]\tLoss: 0.189731\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.451751\n",
      "Train Epoch: 5 [41600/60000 (69%)]\tLoss: 0.138088\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.138624\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.088541\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.265998\n",
      "Train Epoch: 5 [54400/60000 (91%)]\tLoss: 0.058364\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.155985\n",
      "\n",
      "Test set: Average loss: 0.0027, Accuracy: 9499/10000 (95%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.103460\n",
      "Train Epoch: 6 [3200/60000 (5%)]\tLoss: 0.074949\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.188826\n",
      "Train Epoch: 6 [9600/60000 (16%)]\tLoss: 0.095275\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.234024\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 0.316133\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.262739\n",
      "Train Epoch: 6 [22400/60000 (37%)]\tLoss: 0.182142\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.204778\n",
      "Train Epoch: 6 [28800/60000 (48%)]\tLoss: 0.057173\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.097721\n",
      "Train Epoch: 6 [35200/60000 (59%)]\tLoss: 0.141229\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.192048\n",
      "Train Epoch: 6 [41600/60000 (69%)]\tLoss: 0.153965\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.054014\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 0.112975\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.046452\n",
      "Train Epoch: 6 [54400/60000 (91%)]\tLoss: 0.099821\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.101445\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 9552/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.143224\n",
      "Train Epoch: 7 [3200/60000 (5%)]\tLoss: 0.276849\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.102958\n",
      "Train Epoch: 7 [9600/60000 (16%)]\tLoss: 0.127162\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.078325\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 0.184211\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.202895\n",
      "Train Epoch: 7 [22400/60000 (37%)]\tLoss: 0.076816\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.247335\n",
      "Train Epoch: 7 [28800/60000 (48%)]\tLoss: 0.158437\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.078121\n",
      "Train Epoch: 7 [35200/60000 (59%)]\tLoss: 0.095527\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.346699\n",
      "Train Epoch: 7 [41600/60000 (69%)]\tLoss: 0.138284\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.163145\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 0.069335\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.098035\n",
      "Train Epoch: 7 [54400/60000 (91%)]\tLoss: 0.175170\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.181498\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 9607/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.145361\n",
      "Train Epoch: 8 [3200/60000 (5%)]\tLoss: 0.118811\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.089265\n",
      "Train Epoch: 8 [9600/60000 (16%)]\tLoss: 0.272683\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.153476\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 0.110332\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.031446\n",
      "Train Epoch: 8 [22400/60000 (37%)]\tLoss: 0.062860\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.141693\n",
      "Train Epoch: 8 [28800/60000 (48%)]\tLoss: 0.080252\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.146805\n",
      "Train Epoch: 8 [35200/60000 (59%)]\tLoss: 0.084090\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.165522\n",
      "Train Epoch: 8 [41600/60000 (69%)]\tLoss: 0.069199\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.034398\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 0.060667\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.068174\n",
      "Train Epoch: 8 [54400/60000 (91%)]\tLoss: 0.165975\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.117452\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 9595/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                              train = True,\n",
    "                              transform = transforms.ToTensor(),\n",
    "                              download = True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./mnist_data/',\n",
    "                             train = False,\n",
    "                             transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle = False)\n",
    "\n",
    "class NNModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(NNModel, self).__init__()\n",
    "        self.l1 = nn.Linear(784,520)\n",
    "        self.l2 = nn.Linear(520,320)\n",
    "        self.l3 = nn.Linear(320,240)\n",
    "        self.l4 = nn.Linear(240,120)\n",
    "        self.l5 = nn.Linear(120,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1,784) #Flatten:(n, 784)\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l3(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return self.l5(x)\n",
    "    \n",
    "model = NNModel()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx%50 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.\n",
    "                  format(epoch, batch_idx* len(data), len(train_loader.dataset), \n",
    "                         100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        \n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        test_loss += criterion(output, target).data.item()\n",
    "        \n",
    "        pred = output.data.max(1, keepdim = True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.\n",
    "         format(test_loss, correct, len(test_loader.dataset),\n",
    "               100.*correct / len(test_loader.dataset)))\n",
    "    \n",
    "    \n",
    "for epoch in range(1, 9):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:77: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epcoh : 1 [0/60000 (0%)]\tLoss: 2.297312\n",
      "Train Epcoh : 1 [3200/60000 (5%)]\tLoss: 2.295216\n",
      "Train Epcoh : 1 [6400/60000 (11%)]\tLoss: 2.280759\n",
      "Train Epcoh : 1 [9600/60000 (16%)]\tLoss: 2.235453\n",
      "Train Epcoh : 1 [12800/60000 (21%)]\tLoss: 1.835290\n",
      "Train Epcoh : 1 [16000/60000 (27%)]\tLoss: 0.908140\n",
      "Train Epcoh : 1 [19200/60000 (32%)]\tLoss: 0.453202\n",
      "Train Epcoh : 1 [22400/60000 (37%)]\tLoss: 0.589773\n",
      "Train Epcoh : 1 [25600/60000 (43%)]\tLoss: 0.165725\n",
      "Train Epcoh : 1 [28800/60000 (48%)]\tLoss: 0.286789\n",
      "Train Epcoh : 1 [32000/60000 (53%)]\tLoss: 0.127275\n",
      "Train Epcoh : 1 [35200/60000 (59%)]\tLoss: 0.189281\n",
      "Train Epcoh : 1 [38400/60000 (64%)]\tLoss: 0.373725\n",
      "Train Epcoh : 1 [41600/60000 (69%)]\tLoss: 0.311172\n",
      "Train Epcoh : 1 [44800/60000 (75%)]\tLoss: 0.243919\n",
      "Train Epcoh : 1 [48000/60000 (80%)]\tLoss: 0.186592\n",
      "Train Epcoh : 1 [51200/60000 (85%)]\tLoss: 0.310170\n",
      "Train Epcoh : 1 [54400/60000 (91%)]\tLoss: 0.268090\n",
      "Train Epcoh : 1 [57600/60000 (96%)]\tLoss: 0.181770\n",
      "\n",
      "Test set: Average loss: 0.1619, Accuracy: 9512/10000 (951200%)\n",
      "\n",
      "Train Epcoh : 2 [0/60000 (0%)]\tLoss: 0.235521\n",
      "Train Epcoh : 2 [3200/60000 (5%)]\tLoss: 0.235456\n",
      "Train Epcoh : 2 [6400/60000 (11%)]\tLoss: 0.139827\n",
      "Train Epcoh : 2 [9600/60000 (16%)]\tLoss: 0.036820\n",
      "Train Epcoh : 2 [12800/60000 (21%)]\tLoss: 0.249870\n",
      "Train Epcoh : 2 [16000/60000 (27%)]\tLoss: 0.167049\n",
      "Train Epcoh : 2 [19200/60000 (32%)]\tLoss: 0.168694\n",
      "Train Epcoh : 2 [22400/60000 (37%)]\tLoss: 0.167452\n",
      "Train Epcoh : 2 [25600/60000 (43%)]\tLoss: 0.150276\n",
      "Train Epcoh : 2 [28800/60000 (48%)]\tLoss: 0.116539\n",
      "Train Epcoh : 2 [32000/60000 (53%)]\tLoss: 0.165204\n",
      "Train Epcoh : 2 [35200/60000 (59%)]\tLoss: 0.091505\n",
      "Train Epcoh : 2 [38400/60000 (64%)]\tLoss: 0.086267\n",
      "Train Epcoh : 2 [41600/60000 (69%)]\tLoss: 0.104604\n",
      "Train Epcoh : 2 [44800/60000 (75%)]\tLoss: 0.114163\n",
      "Train Epcoh : 2 [48000/60000 (80%)]\tLoss: 0.108129\n",
      "Train Epcoh : 2 [51200/60000 (85%)]\tLoss: 0.162280\n",
      "Train Epcoh : 2 [54400/60000 (91%)]\tLoss: 0.142059\n",
      "Train Epcoh : 2 [57600/60000 (96%)]\tLoss: 0.237282\n",
      "\n",
      "Test set: Average loss: 0.1059, Accuracy: 9664/10000 (966400%)\n",
      "\n",
      "Train Epcoh : 3 [0/60000 (0%)]\tLoss: 0.072965\n",
      "Train Epcoh : 3 [3200/60000 (5%)]\tLoss: 0.049769\n",
      "Train Epcoh : 3 [6400/60000 (11%)]\tLoss: 0.050227\n",
      "Train Epcoh : 3 [9600/60000 (16%)]\tLoss: 0.054862\n",
      "Train Epcoh : 3 [12800/60000 (21%)]\tLoss: 0.206917\n",
      "Train Epcoh : 3 [16000/60000 (27%)]\tLoss: 0.035578\n",
      "Train Epcoh : 3 [19200/60000 (32%)]\tLoss: 0.026674\n",
      "Train Epcoh : 3 [22400/60000 (37%)]\tLoss: 0.044583\n",
      "Train Epcoh : 3 [25600/60000 (43%)]\tLoss: 0.110552\n",
      "Train Epcoh : 3 [28800/60000 (48%)]\tLoss: 0.141814\n",
      "Train Epcoh : 3 [32000/60000 (53%)]\tLoss: 0.030056\n",
      "Train Epcoh : 3 [35200/60000 (59%)]\tLoss: 0.027628\n",
      "Train Epcoh : 3 [38400/60000 (64%)]\tLoss: 0.011463\n",
      "Train Epcoh : 3 [41600/60000 (69%)]\tLoss: 0.136336\n",
      "Train Epcoh : 3 [44800/60000 (75%)]\tLoss: 0.100455\n",
      "Train Epcoh : 3 [48000/60000 (80%)]\tLoss: 0.160526\n",
      "Train Epcoh : 3 [51200/60000 (85%)]\tLoss: 0.022152\n",
      "Train Epcoh : 3 [54400/60000 (91%)]\tLoss: 0.057418\n",
      "Train Epcoh : 3 [57600/60000 (96%)]\tLoss: 0.247438\n",
      "\n",
      "Test set: Average loss: 0.0793, Accuracy: 9750/10000 (975000%)\n",
      "\n",
      "Train Epcoh : 4 [0/60000 (0%)]\tLoss: 0.040770\n",
      "Train Epcoh : 4 [3200/60000 (5%)]\tLoss: 0.031988\n",
      "Train Epcoh : 4 [6400/60000 (11%)]\tLoss: 0.071212\n",
      "Train Epcoh : 4 [9600/60000 (16%)]\tLoss: 0.119764\n",
      "Train Epcoh : 4 [12800/60000 (21%)]\tLoss: 0.087121\n",
      "Train Epcoh : 4 [16000/60000 (27%)]\tLoss: 0.027650\n",
      "Train Epcoh : 4 [19200/60000 (32%)]\tLoss: 0.056885\n",
      "Train Epcoh : 4 [22400/60000 (37%)]\tLoss: 0.057948\n",
      "Train Epcoh : 4 [25600/60000 (43%)]\tLoss: 0.045083\n",
      "Train Epcoh : 4 [28800/60000 (48%)]\tLoss: 0.085019\n",
      "Train Epcoh : 4 [32000/60000 (53%)]\tLoss: 0.053219\n",
      "Train Epcoh : 4 [35200/60000 (59%)]\tLoss: 0.115261\n",
      "Train Epcoh : 4 [38400/60000 (64%)]\tLoss: 0.164562\n",
      "Train Epcoh : 4 [41600/60000 (69%)]\tLoss: 0.049551\n",
      "Train Epcoh : 4 [44800/60000 (75%)]\tLoss: 0.129414\n",
      "Train Epcoh : 4 [48000/60000 (80%)]\tLoss: 0.090379\n",
      "Train Epcoh : 4 [51200/60000 (85%)]\tLoss: 0.039951\n",
      "Train Epcoh : 4 [54400/60000 (91%)]\tLoss: 0.109402\n",
      "Train Epcoh : 4 [57600/60000 (96%)]\tLoss: 0.153709\n",
      "\n",
      "Test set: Average loss: 0.0653, Accuracy: 9801/10000 (980100%)\n",
      "\n",
      "Train Epcoh : 5 [0/60000 (0%)]\tLoss: 0.311497\n",
      "Train Epcoh : 5 [3200/60000 (5%)]\tLoss: 0.137627\n",
      "Train Epcoh : 5 [6400/60000 (11%)]\tLoss: 0.022505\n",
      "Train Epcoh : 5 [9600/60000 (16%)]\tLoss: 0.032661\n",
      "Train Epcoh : 5 [12800/60000 (21%)]\tLoss: 0.035661\n",
      "Train Epcoh : 5 [16000/60000 (27%)]\tLoss: 0.035151\n",
      "Train Epcoh : 5 [19200/60000 (32%)]\tLoss: 0.101532\n",
      "Train Epcoh : 5 [22400/60000 (37%)]\tLoss: 0.110262\n",
      "Train Epcoh : 5 [25600/60000 (43%)]\tLoss: 0.090351\n",
      "Train Epcoh : 5 [28800/60000 (48%)]\tLoss: 0.054776\n",
      "Train Epcoh : 5 [32000/60000 (53%)]\tLoss: 0.219701\n",
      "Train Epcoh : 5 [35200/60000 (59%)]\tLoss: 0.028974\n",
      "Train Epcoh : 5 [38400/60000 (64%)]\tLoss: 0.105222\n",
      "Train Epcoh : 5 [41600/60000 (69%)]\tLoss: 0.035615\n",
      "Train Epcoh : 5 [44800/60000 (75%)]\tLoss: 0.040962\n",
      "Train Epcoh : 5 [48000/60000 (80%)]\tLoss: 0.050070\n",
      "Train Epcoh : 5 [51200/60000 (85%)]\tLoss: 0.069666\n",
      "Train Epcoh : 5 [54400/60000 (91%)]\tLoss: 0.059392\n",
      "Train Epcoh : 5 [57600/60000 (96%)]\tLoss: 0.131062\n",
      "\n",
      "Test set: Average loss: 0.0716, Accuracy: 9765/10000 (976500%)\n",
      "\n",
      "Train Epcoh : 6 [0/60000 (0%)]\tLoss: 0.134728\n",
      "Train Epcoh : 6 [3200/60000 (5%)]\tLoss: 0.015017\n",
      "Train Epcoh : 6 [6400/60000 (11%)]\tLoss: 0.127993\n",
      "Train Epcoh : 6 [9600/60000 (16%)]\tLoss: 0.071647\n",
      "Train Epcoh : 6 [12800/60000 (21%)]\tLoss: 0.026825\n",
      "Train Epcoh : 6 [16000/60000 (27%)]\tLoss: 0.062418\n",
      "Train Epcoh : 6 [19200/60000 (32%)]\tLoss: 0.017422\n",
      "Train Epcoh : 6 [22400/60000 (37%)]\tLoss: 0.056172\n",
      "Train Epcoh : 6 [25600/60000 (43%)]\tLoss: 0.080066\n",
      "Train Epcoh : 6 [28800/60000 (48%)]\tLoss: 0.010517\n",
      "Train Epcoh : 6 [32000/60000 (53%)]\tLoss: 0.093502\n",
      "Train Epcoh : 6 [35200/60000 (59%)]\tLoss: 0.016735\n",
      "Train Epcoh : 6 [38400/60000 (64%)]\tLoss: 0.013689\n",
      "Train Epcoh : 6 [41600/60000 (69%)]\tLoss: 0.021920\n",
      "Train Epcoh : 6 [44800/60000 (75%)]\tLoss: 0.065263\n",
      "Train Epcoh : 6 [48000/60000 (80%)]\tLoss: 0.079362\n",
      "Train Epcoh : 6 [51200/60000 (85%)]\tLoss: 0.148245\n",
      "Train Epcoh : 6 [54400/60000 (91%)]\tLoss: 0.033777\n",
      "Train Epcoh : 6 [57600/60000 (96%)]\tLoss: 0.261489\n",
      "\n",
      "Test set: Average loss: 0.0551, Accuracy: 9835/10000 (983500%)\n",
      "\n",
      "Train Epcoh : 7 [0/60000 (0%)]\tLoss: 0.009415\n",
      "Train Epcoh : 7 [3200/60000 (5%)]\tLoss: 0.024043\n",
      "Train Epcoh : 7 [6400/60000 (11%)]\tLoss: 0.034636\n",
      "Train Epcoh : 7 [9600/60000 (16%)]\tLoss: 0.072179\n",
      "Train Epcoh : 7 [12800/60000 (21%)]\tLoss: 0.036234\n",
      "Train Epcoh : 7 [16000/60000 (27%)]\tLoss: 0.020661\n",
      "Train Epcoh : 7 [19200/60000 (32%)]\tLoss: 0.042971\n",
      "Train Epcoh : 7 [22400/60000 (37%)]\tLoss: 0.082386\n",
      "Train Epcoh : 7 [25600/60000 (43%)]\tLoss: 0.036861\n",
      "Train Epcoh : 7 [28800/60000 (48%)]\tLoss: 0.040711\n",
      "Train Epcoh : 7 [32000/60000 (53%)]\tLoss: 0.054899\n",
      "Train Epcoh : 7 [35200/60000 (59%)]\tLoss: 0.047319\n",
      "Train Epcoh : 7 [38400/60000 (64%)]\tLoss: 0.062901\n",
      "Train Epcoh : 7 [41600/60000 (69%)]\tLoss: 0.082167\n",
      "Train Epcoh : 7 [44800/60000 (75%)]\tLoss: 0.144406\n",
      "Train Epcoh : 7 [48000/60000 (80%)]\tLoss: 0.077020\n",
      "Train Epcoh : 7 [51200/60000 (85%)]\tLoss: 0.029818\n",
      "Train Epcoh : 7 [54400/60000 (91%)]\tLoss: 0.022296\n",
      "Train Epcoh : 7 [57600/60000 (96%)]\tLoss: 0.015239\n",
      "\n",
      "Test set: Average loss: 0.0521, Accuracy: 9828/10000 (982800%)\n",
      "\n",
      "Train Epcoh : 8 [0/60000 (0%)]\tLoss: 0.046990\n",
      "Train Epcoh : 8 [3200/60000 (5%)]\tLoss: 0.036950\n",
      "Train Epcoh : 8 [6400/60000 (11%)]\tLoss: 0.073151\n",
      "Train Epcoh : 8 [9600/60000 (16%)]\tLoss: 0.065038\n",
      "Train Epcoh : 8 [12800/60000 (21%)]\tLoss: 0.086584\n",
      "Train Epcoh : 8 [16000/60000 (27%)]\tLoss: 0.032977\n",
      "Train Epcoh : 8 [19200/60000 (32%)]\tLoss: 0.016264\n",
      "Train Epcoh : 8 [22400/60000 (37%)]\tLoss: 0.093863\n",
      "Train Epcoh : 8 [25600/60000 (43%)]\tLoss: 0.079128\n",
      "Train Epcoh : 8 [28800/60000 (48%)]\tLoss: 0.024802\n",
      "Train Epcoh : 8 [32000/60000 (53%)]\tLoss: 0.068400\n",
      "Train Epcoh : 8 [35200/60000 (59%)]\tLoss: 0.016678\n",
      "Train Epcoh : 8 [38400/60000 (64%)]\tLoss: 0.042041\n",
      "Train Epcoh : 8 [41600/60000 (69%)]\tLoss: 0.026413\n",
      "Train Epcoh : 8 [44800/60000 (75%)]\tLoss: 0.161275\n",
      "Train Epcoh : 8 [48000/60000 (80%)]\tLoss: 0.015025\n",
      "Train Epcoh : 8 [51200/60000 (85%)]\tLoss: 0.098201\n",
      "Train Epcoh : 8 [54400/60000 (91%)]\tLoss: 0.121309\n",
      "Train Epcoh : 8 [57600/60000 (96%)]\tLoss: 0.011564\n",
      "\n",
      "Test set: Average loss: 0.0525, Accuracy: 9837/10000 (983700%)\n",
      "\n",
      "Train Epcoh : 9 [0/60000 (0%)]\tLoss: 0.116228\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epcoh : 9 [3200/60000 (5%)]\tLoss: 0.130043\n",
      "Train Epcoh : 9 [6400/60000 (11%)]\tLoss: 0.085800\n",
      "Train Epcoh : 9 [9600/60000 (16%)]\tLoss: 0.036722\n",
      "Train Epcoh : 9 [12800/60000 (21%)]\tLoss: 0.022263\n",
      "Train Epcoh : 9 [16000/60000 (27%)]\tLoss: 0.029629\n",
      "Train Epcoh : 9 [19200/60000 (32%)]\tLoss: 0.016486\n",
      "Train Epcoh : 9 [22400/60000 (37%)]\tLoss: 0.030139\n",
      "Train Epcoh : 9 [25600/60000 (43%)]\tLoss: 0.051207\n",
      "Train Epcoh : 9 [28800/60000 (48%)]\tLoss: 0.065443\n",
      "Train Epcoh : 9 [32000/60000 (53%)]\tLoss: 0.019198\n",
      "Train Epcoh : 9 [35200/60000 (59%)]\tLoss: 0.051885\n",
      "Train Epcoh : 9 [38400/60000 (64%)]\tLoss: 0.051268\n",
      "Train Epcoh : 9 [41600/60000 (69%)]\tLoss: 0.040315\n",
      "Train Epcoh : 9 [44800/60000 (75%)]\tLoss: 0.011383\n",
      "Train Epcoh : 9 [48000/60000 (80%)]\tLoss: 0.099227\n",
      "Train Epcoh : 9 [51200/60000 (85%)]\tLoss: 0.018509\n",
      "Train Epcoh : 9 [54400/60000 (91%)]\tLoss: 0.076363\n",
      "Train Epcoh : 9 [57600/60000 (96%)]\tLoss: 0.043268\n",
      "\n",
      "Test set: Average loss: 0.0516, Accuracy: 9837/10000 (983700%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = datasets.MNIST(root = './mnist_data/', train = True, transform = transforms.ToTensor(), download = True)\n",
    "test_dataset = datasets.MNIST(root = './mnist_data', train = False, transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset, batch_size = batch_size, shuffle = True)\n",
    "\n",
    "class InceptionModule(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionModule, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        \n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16, 24, kernel_size = 5, padding = 2)\n",
    "        \n",
    "        self.branch3x3_1 = nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        self.branch3x3_2 = nn.Conv2d(16, 24, kernel_size = 3, padding = 1)\n",
    "        self.branch3x3_3 = nn.Conv2d(24, 24, kernel_size = 3, padding = 1)\n",
    "        \n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        \n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "        \n",
    "        branch3x3 = self.branch3x3_1(x)\n",
    "        branch3x3 = self.branch3x3_2(branch3x3)\n",
    "        branch3x3 = self.branch3x3_3(branch3x3)\n",
    "        \n",
    "        branch_pool = F.avg_pool2d(x, kernel_size = 3, stride =1, padding =1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        \n",
    "        outputs = [branch1x1, branch5x5, branch3x3, branch_pool]\n",
    "        \n",
    "        #torch.cat\n",
    "        #x = tensor([[0.6580, -1.0969, -0.4614],\n",
    "        #[-0.1034, -0.5790, 0.1497]])\n",
    "        #torch.cat((x,x,x),0) -> (6, 3) 행렬\n",
    "        #torch.cat((x,x,x),1) -> (2, 9) 행렬로 합쳐짐\n",
    "        #(n*n, 16), (n*n, 24), (n*n, 24), (n*n, 24) -> (n*n, 88)\n",
    "        \n",
    "        return torch.cat(outputs, 1)\n",
    "    \n",
    "class MainNet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MainNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1,10,kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(88,20,kernel_size = 5)\n",
    "        \n",
    "        self.incept1 = InceptionModule(in_channels = 10)\n",
    "        self.incept2 = InceptionModule(in_channels = 20)\n",
    "        \n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0) #size(0) -> (n, 28 * 28) return n\n",
    "        x = F.relu(self.max_pool(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.max_pool(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "model = MainNet().to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 50 == 0:\n",
    "            print('Train Epcoh : {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format\n",
    "                 (epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                 100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "            \n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        output = model(data)\n",
    "        test_loss += criterion(output, target).item()\n",
    "        \n",
    "        pred = output.data.max(1,keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)/batch_size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format\n",
    "         (test_loss, correct, len(test_loader.dataset),\n",
    "         100.*correct, len(test_loader.dataset)))\n",
    "    \n",
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "x_data = [0,1,0,2,3,3] #hihell\n",
    "             \n",
    "one_hot_lookup = [[1, 0, 0, 0, 0],\n",
    "                  [0, 1, 0, 0, 0],\n",
    "                  [0, 0, 1, 0, 0],\n",
    "                  [0, 0, 0, 1, 0],\n",
    "                  [0, 0, 0, 0, 1]]\n",
    "             \n",
    "x_one_hot = [one_hot_lookup[x] for x in x_data]\n",
    "             \n",
    "y_data = [1,0,2,3,3,4]#ihello\n",
    "             \n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5 # one-hote size\n",
    "hidden_size = 5 #output from the cell \n",
    "batch_size = 1 # sentence의 수(단어개수)\n",
    "sequence_length = 1 #한번에 하나씩\n",
    "num_layers = 1\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "    \n",
    "    def forward(self, hidden, x):\n",
    "        #input x를 (batcch_size, sequence_length, input_size)로 reshape 함\n",
    "        #just for make sure\n",
    "        x = x.view(batch_size, sequence_length, input_size)\n",
    "             \n",
    "        #Propagate input through RNN\n",
    "        #Input:(batch, seq_len, input_size)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "             \n",
    "        #for make sure, output이 N * 5 shape을 따르게 하기 위하여\n",
    "        return hidden, out.view(-1, num_classes)\n",
    "             \n",
    "    def init_hidden(self):\n",
    "        #initialize hidden and cell states\n",
    "        #(num_layers * num_directions, batch, hidden_size)\n",
    "        return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n",
    "             \n",
    "model = Model()\n",
    "print(model)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.1)             \n",
    "             \n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    loss = 0\n",
    "    hidden = model.init_hidden()\n",
    "    sys.stdout.write(\"predicted string : \")\n",
    "    \n",
    "    for input, label in zip(inputs, labels):\n",
    "        hidden, output = model(hidden, input)\n",
    "        val, idx = output.max(1)\n",
    "        sys.stdout.write(idx2char[idx.data.item()])\n",
    "        loss += criterion(output, label) # error : dimension specified as 0 but tensor has no dimensions\n",
    "    \n",
    "    print(\", eopch: #d, loss: %1.3f\" % (epoch+1, loss.data.item()))\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(5, 5, batch_first=True)\n",
      ")\n",
      "epoch: 1, loss: 1.693\n",
      "Predicted string:  llllll\n",
      "epoch: 2, loss: 1.523\n",
      "Predicted string:  llllll\n",
      "epoch: 3, loss: 1.393\n",
      "Predicted string:  llllll\n",
      "epoch: 4, loss: 1.263\n",
      "Predicted string:  llllll\n",
      "epoch: 5, loss: 1.146\n",
      "Predicted string:  llllll\n",
      "epoch: 6, loss: 1.055\n",
      "Predicted string:  lhelll\n",
      "epoch: 7, loss: 1.002\n",
      "Predicted string:  ihelll\n",
      "epoch: 8, loss: 0.965\n",
      "Predicted string:  ihelll\n",
      "epoch: 9, loss: 0.913\n",
      "Predicted string:  ihelll\n",
      "epoch: 10, loss: 0.879\n",
      "Predicted string:  ihelll\n",
      "epoch: 11, loss: 0.840\n",
      "Predicted string:  ihelll\n",
      "epoch: 12, loss: 0.805\n",
      "Predicted string:  ihello\n",
      "epoch: 13, loss: 0.779\n",
      "Predicted string:  ihello\n",
      "epoch: 14, loss: 0.758\n",
      "Predicted string:  ihello\n",
      "epoch: 15, loss: 0.738\n",
      "Predicted string:  ihello\n",
      "epoch: 16, loss: 0.717\n",
      "Predicted string:  ihello\n",
      "epoch: 17, loss: 0.694\n",
      "Predicted string:  ihello\n",
      "epoch: 18, loss: 0.667\n",
      "Predicted string:  ihelll\n",
      "epoch: 19, loss: 0.643\n",
      "Predicted string:  ihelll\n",
      "epoch: 20, loss: 0.647\n",
      "Predicted string:  ihelll\n",
      "epoch: 21, loss: 0.628\n",
      "Predicted string:  ihelll\n",
      "epoch: 22, loss: 0.607\n",
      "Predicted string:  ihelll\n",
      "epoch: 23, loss: 0.600\n",
      "Predicted string:  ihelll\n",
      "epoch: 24, loss: 0.596\n",
      "Predicted string:  ihello\n",
      "epoch: 25, loss: 0.591\n",
      "Predicted string:  ihello\n",
      "epoch: 26, loss: 0.583\n",
      "Predicted string:  ihello\n",
      "epoch: 27, loss: 0.573\n",
      "Predicted string:  ihello\n",
      "epoch: 28, loss: 0.562\n",
      "Predicted string:  ihello\n",
      "epoch: 29, loss: 0.550\n",
      "Predicted string:  ihello\n",
      "epoch: 30, loss: 0.540\n",
      "Predicted string:  ihello\n",
      "epoch: 31, loss: 0.527\n",
      "Predicted string:  ihello\n",
      "epoch: 32, loss: 0.524\n",
      "Predicted string:  ihello\n",
      "epoch: 33, loss: 0.530\n",
      "Predicted string:  ihello\n",
      "epoch: 34, loss: 0.519\n",
      "Predicted string:  ihello\n",
      "epoch: 35, loss: 0.507\n",
      "Predicted string:  ihello\n",
      "epoch: 36, loss: 0.503\n",
      "Predicted string:  ihello\n",
      "epoch: 37, loss: 0.503\n",
      "Predicted string:  ihello\n",
      "epoch: 38, loss: 0.500\n",
      "Predicted string:  ihello\n",
      "epoch: 39, loss: 0.496\n",
      "Predicted string:  ihello\n",
      "epoch: 40, loss: 0.494\n",
      "Predicted string:  ihello\n",
      "epoch: 41, loss: 0.493\n",
      "Predicted string:  ihello\n",
      "epoch: 42, loss: 0.492\n",
      "Predicted string:  ihello\n",
      "epoch: 43, loss: 0.488\n",
      "Predicted string:  ihello\n",
      "epoch: 44, loss: 0.484\n",
      "Predicted string:  ihello\n",
      "epoch: 45, loss: 0.481\n",
      "Predicted string:  ihello\n",
      "epoch: 46, loss: 0.481\n",
      "Predicted string:  ihello\n",
      "epoch: 47, loss: 0.480\n",
      "Predicted string:  ihello\n",
      "epoch: 48, loss: 0.477\n",
      "Predicted string:  ihello\n",
      "epoch: 49, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 50, loss: 0.476\n",
      "Predicted string:  ihello\n",
      "epoch: 51, loss: 0.475\n",
      "Predicted string:  ihello\n",
      "epoch: 52, loss: 0.473\n",
      "Predicted string:  ihello\n",
      "epoch: 53, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 54, loss: 0.472\n",
      "Predicted string:  ihello\n",
      "epoch: 55, loss: 0.471\n",
      "Predicted string:  ihello\n",
      "epoch: 56, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 57, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 58, loss: 0.469\n",
      "Predicted string:  ihello\n",
      "epoch: 59, loss: 0.468\n",
      "Predicted string:  ihello\n",
      "epoch: 60, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 61, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 62, loss: 0.467\n",
      "Predicted string:  ihello\n",
      "epoch: 63, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 64, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 65, loss: 0.466\n",
      "Predicted string:  ihello\n",
      "epoch: 66, loss: 0.465\n",
      "Predicted string:  ihello\n",
      "epoch: 67, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 68, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 69, loss: 0.464\n",
      "Predicted string:  ihello\n",
      "epoch: 70, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 71, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 72, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 73, loss: 0.463\n",
      "Predicted string:  ihello\n",
      "epoch: 74, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 75, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 76, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 77, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 78, loss: 0.462\n",
      "Predicted string:  ihello\n",
      "epoch: 79, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 80, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 81, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 82, loss: 0.461\n",
      "Predicted string:  ihello\n",
      "epoch: 83, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 84, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 85, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 86, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 87, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 88, loss: 0.460\n",
      "Predicted string:  ihello\n",
      "epoch: 89, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 90, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 91, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 92, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 93, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 94, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 95, loss: 0.459\n",
      "Predicted string:  ihello\n",
      "epoch: 96, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 97, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 98, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 99, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "epoch: 100, loss: 0.458\n",
      "Predicted string:  ihello\n",
      "Learning finished!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "torch.manual_seed(777)  # reproducibility\n",
    "\n",
    "\n",
    "idx2char = ['h', 'i', 'e', 'l', 'o']\n",
    "\n",
    "# Teach hihell -> ihello\n",
    "x_data = [[0, 1, 0, 2, 3, 3]]   # hihell\n",
    "x_one_hot = [[[1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 1, 0, 0, 0],   # i 1\n",
    "              [1, 0, 0, 0, 0],   # h 0\n",
    "              [0, 0, 1, 0, 0],   # e 2\n",
    "              [0, 0, 0, 1, 0],   # l 3\n",
    "              [0, 0, 0, 1, 0]]]  # l 3\n",
    "\n",
    "y_data = [1, 0, 2, 3, 3, 4]    # ihello\n",
    "\n",
    "# As we have one batch of samples, we will change them to variables only once\n",
    "inputs = Variable(torch.Tensor(x_one_hot))\n",
    "labels = Variable(torch.LongTensor(y_data))\n",
    "\n",
    "num_classes = 5\n",
    "input_size = 5  # one-hot size\n",
    "hidden_size = 5  # output from the LSTM. 5 to directly predict one-hot\n",
    "batch_size = 1   # one sentence\n",
    "sequence_length = 6  # |ihello| == 6\n",
    "num_layers = 1  # one-layer rnn\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        self.num_layers = num_layers\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.rnn = nn.RNN(input_size=5, hidden_size=5, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        # (num_layers * num_directions, batch, hidden_size) for batch_first=True\n",
    "        h_0 = Variable(torch.zeros(\n",
    "            self.num_layers, x.size(0), self.hidden_size))\n",
    "\n",
    "        # Reshape input\n",
    "        x.view(x.size(0), self.sequence_length, self.input_size)\n",
    "\n",
    "        # Propagate input through RNN\n",
    "        # Input: (batch, seq_len, input_size)\n",
    "        # h_0: (num_layers * num_directions, batch, hidden_size)\n",
    "\n",
    "        out, _ = self.rnn(x, h_0)\n",
    "        return out.view(-1, num_classes)\n",
    "\n",
    "\n",
    "# Instantiate RNN model\n",
    "rnn = RNN(num_classes, input_size, hidden_size, num_layers)\n",
    "print(rnn)\n",
    "\n",
    "# Set loss and optimizer function\n",
    "# CrossEntropyLoss = LogSoftmax + NLLLoss\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    outputs = rnn(inputs)\n",
    "    optimizer.zero_grad()\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    _, idx = outputs.max(1)\n",
    "    idx = idx.data.numpy()\n",
    "    result_str = [idx2char[c] for c in idx.squeeze()]\n",
    "    print(\"epoch: %d, loss: %1.3f\" % (epoch + 1, loss.data.item()))\n",
    "    print(\"Predicted string: \", ''.join(result_str))\n",
    "\n",
    "print(\"Learning finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
