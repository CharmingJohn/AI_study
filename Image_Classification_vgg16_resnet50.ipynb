{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_1cRImMSqf-z"
   },
   "source": [
    "#VGG16으로 ImageNet 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSNt07HdAr87"
   },
   "source": [
    "## 이미지 파일 다운로드\n",
    "\n",
    "단지 모델의 동작만 imagenet의 파일로 으로 확인하려고 한다.\n",
    "\n",
    "전체 imagenet은 아주 크다. 몇 개 파일 일부만 다운로드해서 확인한다.\n",
    "\n",
    "각 파일의 url은 http://www.image-net.org/api/text/imagenet.synset.geturls?wnid=n02097474 을 호출하여 구하였다.\n",
    "\n",
    "\n",
    "clsid n07734744은 버섯, n02097474는 강아지, n02123159은 고양이 이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2576
    },
    "colab_type": "code",
    "id": "Wv62ARVGK5yY",
    "outputId": "3af8ef67-8dcf-4c67-f300-0a5850ce6bd3"
   },
   "outputs": [],
   "source": [
    "!rm -rf imagenet\n",
    "!mkdir imagenet\n",
    "\n",
    "# 버섯\n",
    "!wget -O imagenet/mushroom1.jpg http://farm4.static.flickr.com/3023/2822584107_186167ff68.jpg\n",
    "!wget -O imagenet/mushroom2.jpg http://farm3.static.flickr.com/2416/1593642808_efcef6c9c2.jpg\n",
    "!wget -O imagenet/mushroom3.jpg http://farm4.static.flickr.com/3003/2536991564_5f9b2f5b53.jpg\n",
    "  \n",
    "# 강아지\n",
    "!wget -O imagenet/dog1.jpg http://farm1.static.flickr.com/58/160964915_d708f48d0d.jpg\n",
    "!wget -O imagenet/dog2.jpg http://farm1.static.flickr.com/51/144906086_049df05364.jpg\n",
    "!wget -O imagenet/dog3.jpg http://farm3.static.flickr.com/2133/2236535445_ca650757f2.jpg\n",
    "  \n",
    "# 고양이  \n",
    "!wget -O imagenet/cat1.jpg http://farm1.static.flickr.com/131/393656824_bd89c512d0.jpg\n",
    "!wget -O imagenet/cat2.jpg http://farm1.static.flickr.com/213/505539125_d7193beb76.jpg\n",
    "!wget -O imagenet/cat3.jpg http://farm1.static.flickr.com/24/63785988_c16c10b4e5.jpg\n",
    "  \n",
    "  \n",
    "!ls -al imagenet  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EDtuTk-VHEUu"
   },
   "source": [
    "##분류 실행\n",
    "미리 학습된 VGG16 모델을 사용하여 ImageNet 데이터를 대상으로 분류한다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4V7LtoMRmFqJ",
    "outputId": "ddf154e4-b3e9-4005-dbe6-9dee54ff483e"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications import vgg16\n",
    "from IPython.display import display # 이미지 출력 함수\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def predict_vgg16(model, filename) :\n",
    "\n",
    "  # 이미지 파일을 읽고 화면에 표시\n",
    "  image = load_img(filename)\n",
    "  # image = PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=688x550\n",
    "  display(image)\n",
    "\n",
    "  \n",
    "  # 모델 사이즈로 이미지 파일을 읽기\n",
    "  image = load_img(filename, target_size=(224, 224))\n",
    "  # image = PIL.Image.Image image mode=RGB size=224x224\n",
    "\n",
    "  \n",
    "  # 이미지 데이터를 numpy로 변환\n",
    "  image = img_to_array(image)\n",
    "  # [\n",
    "  #   [[211. 184. 163.]\n",
    "  #   [225. 193. 170.]\n",
    "  #   ...\n",
    "  #   [237. 202. 180.]]\n",
    "  #   ...\n",
    "  # ]  \n",
    "  #\n",
    "  # image.shape = (224, 224, 3)\n",
    "\n",
    "  # vgg16.preprocess_input()을 호출하기 위해 차원을 조정\n",
    "  # 보통 모델을 여러 이미지를 한번에 호출. \n",
    "  # 맨 앞의 1 : 이미지 갯수가 1개라는 것.\n",
    "  # 두번째 224 : 가로\n",
    "  # 세번째 224 : 세로\n",
    "  # 네번째 3 : R, G, B 3개\n",
    "  image = image.reshape((1, 224, 224, 3))\n",
    "\n",
    "  # VGG16 모델 호출을 위해 데이터 전처리.\n",
    "  # -255 ~ 255 사이 값으로 정규화한다.\n",
    "  # 그리고 RGB를 BGR순으로 바꾼다.\n",
    "  image = vgg16.preprocess_input(image)\n",
    "  \n",
    "  \n",
    "  # 이미지를 모델에 적용\n",
    "  yhat = model.predict(image)\n",
    "  # yhat = [[2.03485320e-06 4.21382174e-06 1.45730738e-07 1.04057870e-06\n",
    "  #          6.61934010e-08 2.63145339e-04 4.49358195e-05 2.03222541e-08\n",
    "  #          ... ]] # 1000개 클래스에 대한 결과값.\n",
    "  #\n",
    "    \n",
    "    \n",
    "  # 모델 적용된 결과를 파싱\n",
    "  label = vgg16.decode_predictions(yhat)\n",
    "  # label = [[('n02655020', 'puffer', 0.9612253), ... ]]\n",
    "\n",
    "  # 가장 확률이 높은 결과를 획득\n",
    "  label = label[0][0]\n",
    "  # label = ('n02655020', 'puffer', 0.9612253)\n",
    "\n",
    "  # 라벨과 라벨을 예측한 확률을 출력\n",
    "  print('%s (%.2f%%)' % (label[1], label[2]*100))    \n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 4746
    },
    "colab_type": "code",
    "id": "hA8dcLRzmM_2",
    "outputId": "d6e153ba-8c86-4ea8-c3a6-d432f8d2c6c3"
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "# VGG16 모델 불러오기\n",
    "model = vgg16.VGG16()\n",
    "\n",
    "# 모델의 모양을 보여준다.\n",
    "model.summary()\n",
    "\n",
    "# 테스트 할 이미지 파일들\n",
    "files = [\n",
    "    'imagenet/mushroom1.jpg',\n",
    "     'imagenet/mushroom2.jpg',\n",
    "     'imagenet/mushroom3.jpg',\n",
    "     'imagenet/dog1.jpg',\n",
    "     'imagenet/dog2.jpg',\n",
    "     'imagenet/dog3.jpg',\n",
    "     'imagenet/cat1.jpg',\n",
    "     'imagenet/cat2.jpg',\n",
    "     'imagenet/cat3.jpg',\n",
    "    ]\n",
    "\n",
    "# 분류 실행\n",
    "for file in files :\n",
    "  predict_vgg16(model, file)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nryoCveTqxKk"
   },
   "source": [
    "# 커스텀 데이터 분류\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bJwqpDmjX00Y"
   },
   "source": [
    "## 커스텀 데이터 업로드\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "TlfNjtcBgRmC",
    "outputId": "5f5081f4-077f-46b1-83a4-66b51a9fc901"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# 파일이름 dental_image.tar.gz\n",
    "uploaded = files.upload()\n",
    "\n",
    "!ls -al "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3973
    },
    "colab_type": "code",
    "id": "3CzYbC6KkYQV",
    "outputId": "89f72581-661b-4629-a9f8-3a6e3b90f5bc"
   },
   "outputs": [],
   "source": [
    "# 기존 폴더 있으면 삭제\n",
    "!rm -rf dental_image\n",
    "\n",
    "# 압축 파일을 풀기\n",
    "!tar xvfz dental_image.tar.gz\n",
    "\n",
    "!ls -al"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ra5SJ-6XdDc-"
   },
   "source": [
    "## 커스텀 데이타의 분류 실행\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1764
    },
    "colab_type": "code",
    "id": "kezArdORN1aF",
    "outputId": "5fd1b06d-3aea-46cf-e69b-b5fdf2ed5894"
   },
   "outputs": [],
   "source": [
    "from keras.applications import vgg16\n",
    "\n",
    "# VGG16 모델을 불러오기\n",
    "model = vgg16.VGG16()\n",
    "\n",
    "# 모델의 모양을 보여준다.\n",
    "model.summary()\n",
    "  \n",
    "\n",
    "files = [\n",
    "    'dental_image/test/healthy/1.jpg',\n",
    "    'dental_image/test/decayed/101.jpg',\n",
    "    'dental_image/test/cured/301.jpg'\n",
    "        ]\n",
    "\n",
    "  \n",
    "for file in files :\n",
    "  predict_vgg16(model, file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "weav1pp2iIPO"
   },
   "source": [
    "# 커스텀 데이터로 학습\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AhHU96TRrLU5"
   },
   "source": [
    "## 데이터 로더 준비\n",
    "\n",
    "Keras에서 제공하는 ImageDataGenerator를 사용한다.\n",
    "\n",
    "적은 수의 데이터를 커버하기 위하여 데이터 증강을 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "ZtOmy_mUiGk6",
    "outputId": "81ec8cae-ec3e-4ad4-92f6-fb5882de6aad"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = 'dental_image/train'\n",
    "validation_dir = 'dental_image/test'\n",
    "batch_size = 32\n",
    "image_size = 224\n",
    "\n",
    "# 학습에 사용될 이미지 데이터 생성기\n",
    "train_datagen = ImageDataGenerator(\n",
    "      rotation_range=180, # 회전 쵀대 20도\n",
    "      width_shift_range=0.2, # 좌우 이동\n",
    "      height_shift_range=0.2, # 상하 이동\n",
    "      horizontal_flip=True, # 좌우 반전\n",
    "      vertical_flip=True, # 상하 반전\n",
    "      )\n",
    " \n",
    "# 검증에 사용될 이미지 데이터 생성기\n",
    "validation_datagen = ImageDataGenerator()\n",
    " \n",
    "\n",
    "# 학습에 사용될 데이터 생성기  \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "\n",
    "# 검증에 사용될 데이터 생성기\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_num=len(train_generator.class_indices)\n",
    "\n",
    "custom_labels = list(validation_generator.class_indices.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4QhAHlbIkRbJ"
   },
   "source": [
    "## 모델 새로 정의\n",
    "이미 학습된 VGG16의 conv 레이어는 그대로 두고\n",
    "Fully Connected 레이어 부분만 새로 정의한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1160
    },
    "colab_type": "code",
    "id": "kx73nzbjkeD7",
    "outputId": "14960dd9-c280-4e47-f4e2-0e7d34b12337"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session() # 새로운 세션으로 시작\n",
    "\n",
    "from keras.applications import VGG16\n",
    "# 모델 불러오기\n",
    "conv_layers = VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "conv_layers.summary()\n",
    "\n",
    "# Convolution Layer를 학습되지 않도록 고정 \n",
    "for layer in conv_layers.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "# 새로운 모델 생성하기\n",
    "model = models.Sequential()\n",
    "\n",
    "# VGG16모델의 Convolution Layer를 추가\n",
    "model.add(conv_layers)\n",
    " \n",
    "# 모델의 Fully Connected 부분을 재구성\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(class_num, activation='softmax'))\n",
    "\n",
    "# 모델\n",
    "model.summary()\n",
    "\n",
    "\n",
    "\n",
    "vgg16_model_path = 'new_trained_from_vgg16.h5'\n",
    "\n",
    "model.save(vgg16_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rb2bwXln_Rp"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3526
    },
    "colab_type": "code",
    "id": "XVYUT1QDoHbY",
    "outputId": "1844271a-4b0d-424b-ef54-dadb67c3d703"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# 모델 로딩\n",
    "model = load_model(vgg16_model_path)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    " \n",
    "# 모델 저장\n",
    "model.save(vgg16_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fz4Ld9I9vnSn"
   },
   "source": [
    "## 학습 중의 로스와 정확도 보기\n",
    " accuracy와 loss의 그래프를 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "4jy_V9eAvizI",
    "outputId": "bedc059e-fd51-4c75-efff-639a5943232c"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='accuracy')\n",
    "plt.plot(epochs, loss, 'r', label='loss')\n",
    "plt.title('accuracy and loss')\n",
    "plt.legend()\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-ISfgk9Jtldm"
   },
   "source": [
    "## 새로 학습된 모델로 분류 실행\n",
    "\n",
    "학습된 모델 파일을 읽어서 분류를 실행한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rIdcRIG-OVGF"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications import vgg16\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_custom_vgg16(model, filename) :\n",
    "\n",
    "  image = load_img(filename)\n",
    "  display(image)\n",
    "  \n",
    "  image = load_img(filename, target_size=(224, 224))\n",
    "\n",
    "  image = img_to_array(image)\n",
    "  image = image.reshape((1, 224, 224, 3))\n",
    "  \n",
    "  image = vgg16.preprocess_input(image)\n",
    "  \n",
    "  \n",
    "  yhat = model.predict(image)\n",
    "    \n",
    "\n",
    "  # 최대 출력 인덱스를 구한다.\n",
    "  idx=np.argmax(yhat[0])\n",
    "\n",
    "  # 커스텀 레이블을 출력한다.\n",
    "  print('%s (%.2f%%)' % (custom_labels[idx], yhat[0][idx]*100))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "colab_type": "code",
    "id": "w69StwvftpkE",
    "outputId": "ed113425-3602-44a5-9a5e-f3847732859f"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(vgg16_model_path)\n",
    "  \n",
    "files = [\n",
    "    'dental_image/test/healthy/1.jpg',\n",
    "    'dental_image/test/decayed/101.jpg',\n",
    "    'dental_image/test/cured/301.jpg'\n",
    "        ]\n",
    "\n",
    "  \n",
    "for file in files :\n",
    "  predict_custom_vgg16(model, file)     \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n1bCPMljSMZH"
   },
   "source": [
    "# ResNet으로 ImageNet 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M0s4J4n-B9Pd"
   },
   "source": [
    "## 분류 실행\n",
    "\n",
    "실행 방법도 앞의 VGG16과 동일하다.\n",
    "\n",
    "앞의 predict_vgg16()과 전체 구조는 완벽히 동일하다.\n",
    "\n",
    "비교를 위해 커멘트를 전부 삭제하였고, 다른 부분만 커멘트를 달았다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aOAcYuf65JbI"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "#from keras.applications import vgg16\n",
    "from keras.applications import resnet50\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_resnet20(model, filename) :\n",
    "\n",
    "  image = load_img(filename)\n",
    "  display(image)\n",
    "  \n",
    "  image = load_img(filename, target_size=(224, 224))\n",
    "\n",
    "  \n",
    "  image = img_to_array(image)\n",
    "\n",
    "  image = image.reshape((1, 224, 224, 3))\n",
    "\n",
    "#  image = vgg16.preprocess_input(image)\n",
    "  image = resnet50.preprocess_input(image)\n",
    "  \n",
    "  \n",
    "  yhat = model.predict(image)\n",
    "    \n",
    "    \n",
    "#  label = vgg16.decode_predictions(yhat)\n",
    "  label = resnet50.decode_predictions(yhat)\n",
    "\n",
    "  label = label[0][0]\n",
    "  \n",
    "  print('%s (%.2f%%)' % (label[1], label[2]*100))\n",
    "  \n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6784
    },
    "colab_type": "code",
    "id": "19aF-fLL42DJ",
    "outputId": "10ad3267-4383-4b1a-ae20-46c0dff44b64"
   },
   "outputs": [],
   "source": [
    "#from keras.applications import vgg16\n",
    "from keras.applications import resnet50\n",
    "\n",
    "# RestNet50 모델 불러오기\n",
    "#model = vgg16.VGG16()\n",
    "model = resnet50.ResNet50()\n",
    "\n",
    "model.summary()\n",
    "\n",
    "files = ['imagenet/mushroom1.jpg',\n",
    "         'imagenet/mushroom2.jpg',\n",
    "         'imagenet/mushroom3.jpg',\n",
    "         'imagenet/dog1.jpg',\n",
    "         'imagenet/dog2.jpg',\n",
    "         'imagenet/dog3.jpg',\n",
    "         'imagenet/cat1.jpg',\n",
    "         'imagenet/cat2.jpg',\n",
    "         'imagenet/cat3.jpg',\n",
    "        ]\n",
    "\n",
    "for file in files :\n",
    "#  predict_vgg16(model, file)  \n",
    "  predict_resnet50(model, file)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxwnRkJyr5Mc"
   },
   "source": [
    "## 모델 새로 정의\n",
    "\n",
    "내용은 vGG16과 동일하다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 6860
    },
    "colab_type": "code",
    "id": "PQa9gPqovy-p",
    "outputId": "077798ae-4c7f-45ff-fb25-c7defb75f2e2"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "import keras.backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#from keras.applications import vgg16\n",
    "from keras.applications import resnet50\n",
    "\n",
    "#conv_layers = vgg16.VGG16(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "conv_layers = resnet50.ResNet50(weights='imagenet', include_top=False, input_shape=(image_size, image_size, 3))\n",
    "conv_layers.summary()\n",
    "\n",
    "for layer in conv_layers.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(conv_layers)\n",
    " \n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(class_num, activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# vgg16_model_path = 'new_trained_from_vgg16.h5'\n",
    "resnet50_model_path = 'new_trained_from_resnet50.h5'\n",
    "\n",
    "# model.save(vgg16_model_path)\n",
    "model.save(resnet50_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4isq4q-Lxe04"
   },
   "source": [
    "## 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 3578
    },
    "colab_type": "code",
    "id": "H9PaEeUc4GTY",
    "outputId": "f576bc58-80c3-45d5-faf7-bd214d8b5084"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#model = load_model(vgg16_model_path)\n",
    "model = load_model(resnet50_model_path)\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "              metrics=['acc'])\n",
    "\n",
    "\n",
    "history = model.fit_generator(\n",
    "      train_generator,\n",
    "      steps_per_epoch=train_generator.samples/train_generator.batch_size ,\n",
    "      epochs=100,\n",
    "      validation_data=validation_generator,\n",
    "      validation_steps=validation_generator.samples/validation_generator.batch_size,\n",
    "      verbose=1)\n",
    " \n",
    "\n",
    "#model.save(vgg16_model_path)\n",
    "model.save(resnet50_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vW6UV9NYrJHc"
   },
   "source": [
    "## 학습 중의 로스와 정확도 보기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "colab_type": "code",
    "id": "_9IhpdAprBDM",
    "outputId": "fccd0b5f-750d-4374-abc1-e664c2f02e14"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    " \n",
    "epochs = range(len(acc))\n",
    " \n",
    "plt.plot(epochs, acc, 'b', label='accuracy')\n",
    "plt.plot(epochs, loss, 'r', label='loss')\n",
    "plt.title('accuracy and loss')\n",
    "plt.legend()\n",
    "  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LqwbKqtn768t"
   },
   "source": [
    "## 새로 학습된 모델로 분류 실행\n",
    "\n",
    "학습된 모델 파일을 읽어서 분류를 실행한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl_NtCUNb3zb"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "#from keras.applications import vgg16\n",
    "from keras.applications import resnet50\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_custom_resnet50(model, filename) :\n",
    "\n",
    "  image = load_img(filename)\n",
    "  display(image)\n",
    "  \n",
    "  image = load_img(filename, target_size=(224, 224))\n",
    "\n",
    "  image = img_to_array(image)\n",
    "  image = image.reshape((1, 224, 224, 3))\n",
    "  \n",
    "#  image = vgg16.preprocess_input(image)\n",
    "  image = resnet50.preprocess_input(image)\n",
    "  \n",
    "  \n",
    "  yhat = model.predict(image)\n",
    "    \n",
    "\n",
    "  # 최대 출력 인덱스를 구한다.\n",
    "  idx=np.argmax(yhat[0])\n",
    "\n",
    "  # 커스텀 레이블을 출력한다.\n",
    "  print('%s (%.2f%%)' % (custom_labels[idx], yhat[0][idx]*100))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "colab_type": "code",
    "id": "V6XyB_TlMkcm",
    "outputId": "f9b246a0-cc8e-4ac7-d05c-ff04d9cad838"
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model = load_model(resnet50_model_path)\n",
    "  \n",
    "files = [\n",
    "    'dental_image/test/healthy/1.jpg',\n",
    "    'dental_image/test/decayed/101.jpg',\n",
    "    'dental_image/test/cured/301.jpg'\n",
    "        ]\n",
    "\n",
    "  \n",
    "for file in files :\n",
    "  predict_custom_resnet50(model, file)     \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Image_Classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
